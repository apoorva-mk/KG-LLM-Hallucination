{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /var/home/zaristei/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /var/home/zaristei/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /var/home/zaristei/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /var/home/zaristei/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /var/home/zaristei/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# import graphvite as gv\n",
    "import dataset\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"simple_wikidata5m.pkl\", \"rb\")\n",
    "model = pickle.load(file)\n",
    "entity2id = model.graph.entity2id\n",
    "relation2id = model.graph.relation2id\n",
    "entity_embeddings = model.solver.entity_embeddings\n",
    "relation_embeddings = model.solver.relation_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alias2entity = dataset.wikidata5m.load_alias(\"entity.txt.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2580138\n"
     ]
    }
   ],
   "source": [
    "print(entity2id[alias2entity[\"invented\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who invented machine learning, was it Steve Jobs\n",
      "Steve Jobs PERSON\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm') \n",
    "sentence = \"Who invented machine learning, was it Steve Jobs\"\n",
    "doc = nlp(sentence) \n",
    "print(doc)\n",
    "for ent in doc.ents: \n",
    "    print(ent.text, ent.label_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text: invented machine learning steve jobs\n",
      "Named Entities: (Steve Jobs,)\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    # Extract named entities\n",
    "    nlp = spacy.load('en_core_web_sm') \n",
    "    named_entities = nlp(text).ents\n",
    "\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    # Part-of-speech tagging\n",
    "    tagged_words = pos_tag(words)\n",
    "\n",
    "    cleaned_text = ' '.join(words).lower()\n",
    "    \n",
    "    return cleaned_text, named_entities\n",
    "\n",
    "# Example usage:\n",
    "text = \"Who invented machine learning, was it Steve Jobs?\"\n",
    "cleaned_text, named_entities = clean_text(text)\n",
    "print(\"Cleaned Text:\", cleaned_text)\n",
    "print(\"Named Entities:\", named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_continuous_word_sets(text, window_size):\n",
    "    words = text.split()\n",
    "    word_sets = []\n",
    "    \n",
    "    for i in range(len(words) - window_size + 1):\n",
    "        word_set = ' '.join(words[i:i + window_size])\n",
    "        word_sets.append(word_set)\n",
    "\n",
    "    return word_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_to_kg_embeddings(query):\n",
    "    wiki_embeddings = {}\n",
    "    cleaned_text, named_entities = clean_text(query)\n",
    "    for ne in named_entities: \n",
    "        try:\n",
    "            ne = ne.orth_.lower()\n",
    "            wiki_embeddings[ne] = entity_embeddings[entity2id[alias2entity[ne]]]\n",
    "            cleaned_text = cleaned_text.replace(ne, \"\")\n",
    "        except Exception as e:\n",
    "            print(\"KeyError: \", e)\n",
    "    num_of_words = len(word_tokenize(cleaned_text))\n",
    "    for i in range(num_of_words, 0, -1):\n",
    "        word_sets = extract_continuous_word_sets(cleaned_text, i)\n",
    "        for word in word_sets:\n",
    "            try:\n",
    "                wiki_embeddings[word] = entity_embeddings[entity2id[alias2entity[word]]]\n",
    "                cleaned_text = cleaned_text.replace(word, \"\")\n",
    "            except Exception as e:\n",
    "                print(\"KeyError: \", e)\n",
    "\n",
    "    return wiki_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_to_node_id(query):\n",
    "    node_ids = {}\n",
    "    cleaned_text, named_entities = clean_text(query)\n",
    "    for ne in named_entities: \n",
    "        try:\n",
    "            ne = ne.orth_.lower()\n",
    "            node_ids[ne] = alias2entity[ne]\n",
    "            cleaned_text = cleaned_text.replace(ne, \"\")\n",
    "        except Exception as e:\n",
    "            pass\n",
    "            #print(\"KeyError: \", e)\n",
    "    num_of_words = len(word_tokenize(cleaned_text))\n",
    "    for i in range(num_of_words, 0, -1):\n",
    "        word_sets = extract_continuous_word_sets(cleaned_text, i)\n",
    "        for word in word_sets:\n",
    "            try:\n",
    "                node_ids[word] = alias2entity[word]\n",
    "                cleaned_text = cleaned_text.replace(word, \"\")\n",
    "            except Exception as e:\n",
    "                pass\n",
    "                #print(\"KeyError: \", e)\n",
    "    return node_ids    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'isaac newton': 'Q935', 'machine learning': 'Q2539', 'invented': 'Q18119757'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_ids = (query_to_node_id(\"Who invented machine learning, was it Isaac Newton?\"))\n",
    "node_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/home/zaristei/repos/KG-LLM-Hallucination/submodules/HaluEval/data/qa_data.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "halueval_path = os.path.join(os.getcwd(), \"submodules\", \"HaluEval\", \"data\", \"qa_data.json\")\n",
    "print(halueval_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with(open(halueval_path, \"r\")) as f:\n",
    "    data_lst = f.readlines()\n",
    "\n",
    "data_lst = [json.loads(data) for data in data_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELEVANT_KEYS = ['knowledge', 'question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    result = {}\n",
    "    for k in RELEVANT_KEYS:\n",
    "        node_vals = query_to_node_id(data[k])\n",
    "        result.update(node_vals)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = [process_data(i) for i in data_lst[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.contrib.concurrent import process_map\n",
    "import multiprocessing as mp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15925/3956094896.py:2: TqdmWarning: Iterable length 10000 > 1000 but `chunksize` is not set. This may seriously degrade multiprocess performance. Set `chunksize=1` or more.\n",
      "  result = process_map(process_data, data_lst, max_workers=5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3057d61368453fa06137a23f5f7860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "949.5897569656372\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "result = process_map(process_data, data_lst, max_workers=5)\n",
    "'''\n",
    "result = []\n",
    "for data in tqdm.tqdm(data_lst):\n",
    "    result.append(process_data(data))\n",
    "'''\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15925/2795308879.py:4: TqdmWarning: Iterable length 10000 > 1000 but `chunksize` is not set. This may seriously degrade multiprocess performance. Set `chunksize=1` or more.\n",
      "  result = process_map(process_data, data_lst)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8db7c6061b24f1da15fd76db1002195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.contrib.concurrent import thread_map\n",
    "start = time.time()\n",
    "if __name__ == \"__main__\":\n",
    "    result = process_map(process_data, data_lst)\n",
    "#result = []\n",
    "#for data in tqdm.tqdm(data_lst):\n",
    "#    result.append(process_data(data))\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(result, open(\"processed_node_ids.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kghalu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
